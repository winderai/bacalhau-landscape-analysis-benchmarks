{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac856112-e234-4012-8c66-56dd3b5c7b0c",
   "metadata": {},
   "source": [
    "# Average House Price in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ccf4a-2f34-4577-ab51-1f6250bb9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ../../../benchmark/word-count/spark/install-sbt.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406ccd89-9122-4e4a-906a-a1381b038a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg\u001b[39;49;00m.\u001b[04m\u001b[36mapache\u001b[39;49;00m.\u001b[04m\u001b[36mspark\u001b[39;49;00m.\u001b[04m\u001b[32mSparkConf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg\u001b[39;49;00m.\u001b[04m\u001b[36mapache\u001b[39;49;00m.\u001b[04m\u001b[36mspark\u001b[39;49;00m.\u001b[04m\u001b[36msql\u001b[39;49;00m.{\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m, \u001b[04m\u001b[32mSparkSession\u001b[39;49;00m}\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg\u001b[39;49;00m.\u001b[04m\u001b[36mapache\u001b[39;49;00m.\u001b[04m\u001b[36mspark\u001b[39;49;00m.\u001b[04m\u001b[36msql\u001b[39;49;00m.\u001b[04m\u001b[36mfunctions\u001b[39;49;00m._\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg\u001b[39;49;00m.\u001b[04m\u001b[36mapache\u001b[39;49;00m.\u001b[04m\u001b[36mspark\u001b[39;49;00m.\u001b[04m\u001b[36msql\u001b[39;49;00m.\u001b[04m\u001b[36mtypes\u001b[39;49;00m._\n",
      "\n",
      "\u001b[34mobject\u001b[39;49;00m \u001b[04m\u001b[32mAvgPrice\u001b[39;49;00m\n",
      "{\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(args: \u001b[04m\u001b[32mArray\u001b[39;49;00m[\u001b[04m\u001b[32mString\u001b[39;49;00m]): \u001b[04m\u001b[32mUnit\u001b[39;49;00m =\n",
      "    {\n",
      "        \u001b[34mval\u001b[39;49;00m conf = \u001b[34mnew\u001b[39;49;00m \u001b[04m\u001b[32mSparkConf\u001b[39;49;00m().setAppName(\u001b[33m\"Score Computation\"\u001b[39;49;00m).setMaster(\u001b[33m\"local\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mval\u001b[39;49;00m spark = \u001b[04m\u001b[32mSparkSession\u001b[39;49;00m.builder.config(conf).getOrCreate()\n",
      "        \n",
      "        \u001b[34mval\u001b[39;49;00m input_df = spark.read.option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[33m\"true\"\u001b[39;49;00m)\n",
      "          .csv(\u001b[33m\"file://\"\u001b[39;49;00m + \u001b[04m\u001b[32mSystem\u001b[39;49;00m.getProperty(\u001b[33m\"user.dir\"\u001b[39;49;00m) + \u001b[33m\"/../data/address_book.csv\"\u001b[39;49;00m)\n",
      "          .withColumn(\u001b[33m\"Price\"\u001b[39;49;00m, col(\u001b[33m\"Price\"\u001b[39;49;00m).cast(\u001b[04m\u001b[32mDoubleType\u001b[39;49;00m))\n",
      "          .groupBy(\u001b[33m\"Zipcode\"\u001b[39;49;00m)\n",
      "          .agg(avg(\u001b[33m\"Price\"\u001b[39;49;00m).as(\u001b[33m\"AvgPrice\"\u001b[39;49;00m))\n",
      "          .repartition(\u001b[34m1\u001b[39;49;00m)   \u001b[37m// save output to a single file\u001b[39;49;00m\n",
      "          .write\n",
      "          .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\n",
      "          .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[33m\"true\"\u001b[39;49;00m)\n",
      "          .csv(\u001b[33m\"file://\"\u001b[39;49;00m + \u001b[04m\u001b[32mSystem\u001b[39;49;00m.getProperty(\u001b[33m\"user.dir\"\u001b[39;49;00m) + \u001b[33m\"/output\"\u001b[39;49;00m)\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/main/scala/AveragePrice.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213d1033-010d-4758-bb4c-2136fcd9cbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mwelcome to sbt 1.6.2 (Private Build Java 1.8.0_312)\u001b[0m\u001b[0J\n",
      "\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mloading project definition from /home/ubuntu/sample-code-benchmark/sample-code/average-house-price/spark/project\u001b[0m\u001b[0J\n",
      "\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark-build / dependencyPositions 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark-build / Compile / compileIncremental 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mloading settings for project spark from build.sbt ...\u001b[0m\u001b[0J\n",
      "\u001b[0J\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mset current project to AvgPrice (in build file:/home/ubuntu/sample-code-benchmark/sample-code/average-house-price/spark/)\u001b[0m\u001b[0J\n",
      "\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[0J\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / update 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / update 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / Compile / compileIncremental 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[0m[\u001b[0m\u001b[0minfo\u001b[0m] \u001b[0m\u001b[0mcompiling 1 Scala source to /home/ubuntu/sample-code-benchmark/sample-code/average-house-price/spark/target/scala-2.12/classes ...\u001b[0m\u001b[0J\n",
      "\u001b[0J\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / Compile / compileIncremental 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / Compile / compileIncremental 0s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / Compile / compileIncremental 1s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / Compile / compileIncremental 2s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[1000D\n",
      "\u001b[2K\u001b[2K  | => spark / Compile / compileIncremental 3s\n",
      "\u001b[2K\u001b[2A\u001b[1000D\u001b[0J\u001b[0m[\u001b[0m\u001b[32msuccess\u001b[0m] \u001b[0m\u001b[0mTotal time: 6 s, completed Mar 21, 2022 2:30:54 PM\u001b[0m\u001b[0J\n",
      "\u001b[0J\u001b[0J"
     ]
    }
   ],
   "source": [
    "!rm -rf src/main/scala/.ipynb_checkpoints/ && sbt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86da74e8-de8b-497b-8d6a-970d184c822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/03/21 14:30:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/03/21 14:30:56 INFO SparkContext: Running Spark version 3.2.1\n",
      "22/03/21 14:30:56 INFO ResourceUtils: ==============================================================\n",
      "22/03/21 14:30:56 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/03/21 14:30:56 INFO ResourceUtils: ==============================================================\n",
      "22/03/21 14:30:56 INFO SparkContext: Submitted application: Score Computation\n",
      "22/03/21 14:30:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/03/21 14:30:57 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/03/21 14:30:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/03/21 14:30:57 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "22/03/21 14:30:57 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "22/03/21 14:30:57 INFO SecurityManager: Changing view acls groups to: \n",
      "22/03/21 14:30:57 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/03/21 14:30:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "22/03/21 14:30:57 INFO Utils: Successfully started service 'sparkDriver' on port 42103.\n",
      "22/03/21 14:30:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/03/21 14:30:57 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/03/21 14:30:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/03/21 14:30:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/03/21 14:30:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/03/21 14:30:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-29db0151-33bc-48be-a27a-5a992280bda0\n",
      "22/03/21 14:30:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "22/03/21 14:30:57 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/03/21 14:30:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/03/21 14:30:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-12-99.eu-central-1.compute.internal:4040\n",
      "22/03/21 14:30:57 INFO SparkContext: Added JAR file:/home/ubuntu/sample-code-benchmark/sample-code/average-house-price/spark/target/scala-2.12/avgprice_2.12-0.1.jar at spark://ip-172-31-12-99.eu-central-1.compute.internal:42103/jars/avgprice_2.12-0.1.jar with timestamp 1647873056941\n",
      "22/03/21 14:30:58 INFO Executor: Starting executor ID driver on host ip-172-31-12-99.eu-central-1.compute.internal\n",
      "22/03/21 14:30:58 INFO Executor: Fetching spark://ip-172-31-12-99.eu-central-1.compute.internal:42103/jars/avgprice_2.12-0.1.jar with timestamp 1647873056941\n",
      "22/03/21 14:30:58 INFO TransportClientFactory: Successfully created connection to ip-172-31-12-99.eu-central-1.compute.internal/172.31.12.99:42103 after 40 ms (0 ms spent in bootstraps)\n",
      "22/03/21 14:30:58 INFO Utils: Fetching spark://ip-172-31-12-99.eu-central-1.compute.internal:42103/jars/avgprice_2.12-0.1.jar to /tmp/spark-a5ad704a-cd7d-4974-8a2f-241528f15cf6/userFiles-27f743a6-a8c2-48c3-b844-2e7d2755efcb/fetchFileTemp936625011408201001.tmp\n",
      "22/03/21 14:30:58 INFO Executor: Adding file:/tmp/spark-a5ad704a-cd7d-4974-8a2f-241528f15cf6/userFiles-27f743a6-a8c2-48c3-b844-2e7d2755efcb/avgprice_2.12-0.1.jar to class loader\n",
      "22/03/21 14:30:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46453.\n",
      "22/03/21 14:30:58 INFO NettyBlockTransferService: Server created on ip-172-31-12-99.eu-central-1.compute.internal:46453\n",
      "22/03/21 14:30:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/03/21 14:30:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46453, None)\n",
      "22/03/21 14:30:58 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-12-99.eu-central-1.compute.internal:46453 with 366.3 MiB RAM, BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46453, None)\n",
      "22/03/21 14:30:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46453, None)\n",
      "22/03/21 14:30:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46453, None)\n",
      "22/03/21 14:30:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/03/21 14:30:58 INFO SharedState: Warehouse path is 'file:/home/ubuntu/sample-code-benchmark/sample-code/average-house-price/spark/spark-warehouse'.\n",
      "22/03/21 14:30:59 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.\n",
      "22/03/21 14:30:59 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "22/03/21 14:31:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/03/21 14:31:02 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "22/03/21 14:31:02 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/03/21 14:31:02 INFO CodeGenerator: Code generated in 201.918292 ms\n",
      "22/03/21 14:31:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.7 KiB, free 366.0 MiB)\n",
      "22/03/21 14:31:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)\n",
      "22/03/21 14:31:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 32.7 KiB, free: 366.3 MiB)\n",
      "22/03/21 14:31:02 INFO SparkContext: Created broadcast 0 from csv at AveragePrice.scala:15\n",
      "22/03/21 14:31:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195236 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/03/21 14:31:03 INFO SparkContext: Starting job: csv at AveragePrice.scala:15\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Got job 0 (csv at AveragePrice.scala:15) with 1 output partitions\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Final stage: ResultStage 0 (csv at AveragePrice.scala:15)\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at AveragePrice.scala:15), which has no missing parents\n",
      "22/03/21 14:31:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)\n",
      "22/03/21 14:31:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "22/03/21 14:31:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "22/03/21 14:31:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at AveragePrice.scala:15) (first 15 tasks are for partitions Vector(0))\n",
      "22/03/21 14:31:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/03/21 14:31:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-12-99.eu-central-1.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 4920 bytes) taskResourceAssignments Map()\n",
      "22/03/21 14:31:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/03/21 14:31:03 INFO FileScanRDD: Reading File path: file:///home/ubuntu/sample-code-benchmark/sample-code/average-house-price/data/address_book.csv, range: 0-932, partition values: [empty row]\n",
      "22/03/21 14:31:03 INFO CodeGenerator: Code generated in 27.811903 ms\n",
      "22/03/21 14:31:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1542 bytes result sent to driver\n",
      "22/03/21 14:31:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 449 ms on ip-172-31-12-99.eu-central-1.compute.internal (executor driver) (1/1)\n",
      "22/03/21 14:31:03 INFO DAGScheduler: ResultStage 0 (csv at AveragePrice.scala:15) finished in 0.621 s\n",
      "22/03/21 14:31:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/03/21 14:31:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/21 14:31:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/03/21 14:31:03 INFO DAGScheduler: Job 0 finished: csv at AveragePrice.scala:15, took 0.696154 s\n",
      "22/03/21 14:31:03 INFO CodeGenerator: Code generated in 13.924844 ms\n",
      "22/03/21 14:31:03 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/03/21 14:31:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/03/21 14:31:03 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/03/21 14:31:03 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 338.7 KiB, free 365.6 MiB)\n",
      "22/03/21 14:31:03 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.6 MiB)\n",
      "22/03/21 14:31:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "22/03/21 14:31:04 INFO SparkContext: Created broadcast 2 from csv at AveragePrice.scala:15\n",
      "22/03/21 14:31:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195236 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/03/21 14:31:04 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/03/21 14:31:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/03/21 14:31:04 INFO FileSourceStrategy: Output Data Schema: struct<Zipcode: string, Price: string>\n",
      "22/03/21 14:31:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/03/21 14:31:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/03/21 14:31:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/03/21 14:31:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ip-172-31-12-99.eu-central-1.compute.internal:46453 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "22/03/21 14:31:04 INFO CodeGenerator: Code generated in 130.194624 ms\n",
      "22/03/21 14:31:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 338.6 KiB, free 365.2 MiB)\n",
      "22/03/21 14:31:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.2 MiB)\n",
      "22/03/21 14:31:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 32.7 KiB, free: 366.2 MiB)\n",
      "22/03/21 14:31:04 INFO SparkContext: Created broadcast 3 from csv at AveragePrice.scala:23\n",
      "22/03/21 14:31:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195236 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Registering RDD 13 (csv at AveragePrice.scala:23) as input to shuffle 0\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Got map stage job 1 (csv at AveragePrice.scala:23) with 1 output partitions\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (csv at AveragePrice.scala:23)\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at AveragePrice.scala:23), which has no missing parents\n",
      "22/03/21 14:31:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 39.0 KiB, free 365.2 MiB)\n",
      "22/03/21 14:31:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 365.2 MiB)\n",
      "22/03/21 14:31:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 18.5 KiB, free: 366.2 MiB)\n",
      "22/03/21 14:31:04 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478\n",
      "22/03/21 14:31:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at csv at AveragePrice.scala:23) (first 15 tasks are for partitions Vector(0))\n",
      "22/03/21 14:31:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/03/21 14:31:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-12-99.eu-central-1.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes) taskResourceAssignments Map()\n",
      "22/03/21 14:31:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/03/21 14:31:04 INFO CodeGenerator: Code generated in 22.690792 ms\n",
      "22/03/21 14:31:04 INFO CodeGenerator: Code generated in 16.939192 ms\n",
      "22/03/21 14:31:04 INFO CodeGenerator: Code generated in 15.874023 ms\n",
      "22/03/21 14:31:04 INFO FileScanRDD: Reading File path: file:///home/ubuntu/sample-code-benchmark/sample-code/average-house-price/data/address_book.csv, range: 0-932, partition values: [empty row]\n",
      "22/03/21 14:31:04 INFO CodeGenerator: Code generated in 15.995517 ms\n",
      "22/03/21 14:31:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2798 bytes result sent to driver\n",
      "22/03/21 14:31:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 274 ms on ip-172-31-12-99.eu-central-1.compute.internal (executor driver) (1/1)\n",
      "22/03/21 14:31:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/03/21 14:31:04 INFO DAGScheduler: ShuffleMapStage 1 (csv at AveragePrice.scala:23) finished in 0.304 s\n",
      "22/03/21 14:31:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/03/21 14:31:05 INFO DAGScheduler: running: Set()\n",
      "22/03/21 14:31:05 INFO DAGScheduler: waiting: Set()\n",
      "22/03/21 14:31:05 INFO DAGScheduler: failed: Set()\n",
      "22/03/21 14:31:05 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/03/21 14:31:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/03/21 14:31:05 INFO CodeGenerator: Code generated in 24.585706 ms\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Registering RDD 16 (csv at AveragePrice.scala:23) as input to shuffle 1\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Got map stage job 2 (csv at AveragePrice.scala:23) with 1 output partitions\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (csv at AveragePrice.scala:23)\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[16] at csv at AveragePrice.scala:23), which has no missing parents\n",
      "22/03/21 14:31:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 39.3 KiB, free 365.1 MiB)\n",
      "22/03/21 14:31:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 365.1 MiB)\n",
      "22/03/21 14:31:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 18.9 KiB, free: 366.2 MiB)\n",
      "22/03/21 14:31:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[16] at csv at AveragePrice.scala:23) (first 15 tasks are for partitions Vector(0))\n",
      "22/03/21 14:31:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "22/03/21 14:31:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (ip-172-31-12-99.eu-central-1.compute.internal, executor driver, partition 0, NODE_LOCAL, 4442 bytes) taskResourceAssignments Map()\n",
      "22/03/21 14:31:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "22/03/21 14:31:05 INFO ShuffleBlockFetcherIterator: Getting 1 (400.0 B) non-empty blocks including 1 (400.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/03/21 14:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "22/03/21 14:31:05 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4001 bytes result sent to driver\n",
      "22/03/21 14:31:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 79 ms on ip-172-31-12-99.eu-central-1.compute.internal (executor driver) (1/1)\n",
      "22/03/21 14:31:05 INFO DAGScheduler: ShuffleMapStage 3 (csv at AveragePrice.scala:23) finished in 0.094 s\n",
      "22/03/21 14:31:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/03/21 14:31:05 INFO DAGScheduler: running: Set()\n",
      "22/03/21 14:31:05 INFO DAGScheduler: waiting: Set()\n",
      "22/03/21 14:31:05 INFO DAGScheduler: failed: Set()\n",
      "22/03/21 14:31:05 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "22/03/21 14:31:05 INFO SparkContext: Starting job: csv at AveragePrice.scala:23\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Got job 3 (csv at AveragePrice.scala:23) with 1 output partitions\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Final stage: ResultStage 6 (csv at AveragePrice.scala:23)\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[17] at csv at AveragePrice.scala:23), which has no missing parents\n",
      "22/03/21 14:31:05 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 202.7 KiB, free 364.9 MiB)\n",
      "22/03/21 14:31:05 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 73.0 KiB, free 364.8 MiB)\n",
      "22/03/21 14:31:05 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46453 (size: 73.0 KiB, free: 366.1 MiB)\n",
      "22/03/21 14:31:05 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1478\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[17] at csv at AveragePrice.scala:23) (first 15 tasks are for partitions Vector(0))\n",
      "22/03/21 14:31:05 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "22/03/21 14:31:05 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (ip-172-31-12-99.eu-central-1.compute.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/03/21 14:31:05 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)\n",
      "22/03/21 14:31:05 INFO ShuffleBlockFetcherIterator: Getting 1 (129.0 B) non-empty blocks including 1 (129.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/03/21 14:31:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "22/03/21 14:31:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/03/21 14:31:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/03/21 14:31:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/03/21 14:31:05 INFO FileOutputCommitter: Saved output of task 'attempt_202203211431054469342286486466899_0006_m_000000_3' to file:/home/ubuntu/sample-code-benchmark/sample-code/average-house-price/spark/output/_temporary/0/task_202203211431054469342286486466899_0006_m_000000\n",
      "22/03/21 14:31:05 INFO SparkHadoopMapRedUtil: attempt_202203211431054469342286486466899_0006_m_000000_3: Committed\n",
      "22/03/21 14:31:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 3483 bytes result sent to driver\n",
      "22/03/21 14:31:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 111 ms on ip-172-31-12-99.eu-central-1.compute.internal (executor driver) (1/1)\n",
      "22/03/21 14:31:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "22/03/21 14:31:05 INFO DAGScheduler: ResultStage 6 (csv at AveragePrice.scala:23) finished in 0.142 s\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/21 14:31:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "22/03/21 14:31:05 INFO DAGScheduler: Job 3 finished: csv at AveragePrice.scala:23, took 0.149788 s\n",
      "22/03/21 14:31:05 INFO FileFormatWriter: Start to commit write Job 1a756439-1892-423a-bcd9-550d271a5b6f.\n",
      "22/03/21 14:31:05 INFO FileFormatWriter: Write Job 1a756439-1892-423a-bcd9-550d271a5b6f committed. Elapsed time: 25 ms.\n",
      "22/03/21 14:31:05 INFO FileFormatWriter: Finished processing stats for write job 1a756439-1892-423a-bcd9-550d271a5b6f.\n",
      "22/03/21 14:31:05 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "22/03/21 14:31:05 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-12-99.eu-central-1.compute.internal:4040\n",
      "22/03/21 14:31:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/03/21 14:31:05 INFO MemoryStore: MemoryStore cleared\n",
      "22/03/21 14:31:05 INFO BlockManager: BlockManager stopped\n",
      "22/03/21 14:31:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/03/21 14:31:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/03/21 14:31:05 INFO SparkContext: Successfully stopped SparkContext\n",
      "22/03/21 14:31:05 INFO ShutdownHookManager: Shutdown hook called\n",
      "22/03/21 14:31:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b09d885-5c5a-4701-8d3f-158c761254a0\n",
      "22/03/21 14:31:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5ad704a-cd7d-4974-8a2f-241528f15cf6\n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/bin/spark-submit --master local target/scala-2.12/avgprice_2.12-0.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07df087-b8e7-452f-8905-1af965a4b903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipcode,AvgPrice\n",
      "60173,43300.0\n",
      "28202,44750.0\n",
      "19044,30580.25\n",
      "21076,27875.0\n",
      "90670,56277.5\n"
     ]
    }
   ],
   "source": [
    "!head ./output/part-00000-7af5755b-11aa-4ede-90be-f580a275aed3-c000.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
