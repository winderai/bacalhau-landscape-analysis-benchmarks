{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3952386e-1407-479a-ad08-17fd85971f53",
   "metadata": {},
   "source": [
    "# Spark Word Count\n",
    "\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d6cd4-8e5a-488d-a408-c62466da2cd3",
   "metadata": {},
   "source": [
    "```java\n",
    "package org.apache.spark.examples;\n",
    "\n",
    "import scala.Tuple2;\n",
    "\n",
    "import org.apache.spark.api.java.JavaPairRDD;\n",
    "import org.apache.spark.api.java.JavaRDD;\n",
    "import org.apache.spark.sql.SparkSession;\n",
    "\n",
    "import java.util.Arrays;\n",
    "import java.util.List;\n",
    "import java.util.regex.Pattern;\n",
    "\n",
    "public final class JavaWordCount {\n",
    "  private static final Pattern SPACE = Pattern.compile(\" \");\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "\n",
    "    if (args.length < 1) {\n",
    "      System.err.println(\"Usage: JavaWordCount <file>\");\n",
    "      System.exit(1);\n",
    "    }\n",
    "\n",
    "    SparkSession spark = SparkSession\n",
    "      .builder()\n",
    "      .appName(\"JavaWordCount\")\n",
    "      .getOrCreate();\n",
    "\n",
    "    JavaRDD<String> lines = spark.read().textFile(args[0]).javaRDD();\n",
    "\n",
    "    JavaRDD<String> words = lines.flatMap(s -> Arrays.asList(SPACE.split(s)).iterator());\n",
    "\n",
    "    JavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(s, 1));\n",
    "\n",
    "    JavaPairRDD<String, Integer> counts = ones.reduceByKey((i1, i2) -> i1 + i2);\n",
    "\n",
    "    List<Tuple2<String, Integer>> output = counts.collect();\n",
    "    for (Tuple2<?,?> tuple : output) {\n",
    "      System.out.println(tuple._1() + \": \" + tuple._2());\n",
    "    }\n",
    "    spark.stop();\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbdf5f4d-9fe0-4cef-a988-ba14f8b178d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/03/21 09:25:20 INFO SparkContext: Running Spark version 3.2.1\n",
      "22/03/21 09:25:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/21 09:25:21 INFO ResourceUtils: ==============================================================\n",
      "22/03/21 09:25:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/03/21 09:25:21 INFO ResourceUtils: ==============================================================\n",
      "22/03/21 09:25:21 INFO SparkContext: Submitted application: JavaWordCount\n",
      "22/03/21 09:25:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/03/21 09:25:21 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/03/21 09:25:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/03/21 09:25:21 INFO SecurityManager: Changing view acls to: ubuntu\n",
      "22/03/21 09:25:21 INFO SecurityManager: Changing modify acls to: ubuntu\n",
      "22/03/21 09:25:21 INFO SecurityManager: Changing view acls groups to: \n",
      "22/03/21 09:25:21 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/03/21 09:25:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "22/03/21 09:25:21 INFO Utils: Successfully started service 'sparkDriver' on port 39479.\n",
      "22/03/21 09:25:21 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/03/21 09:25:21 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/03/21 09:25:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/03/21 09:25:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/03/21 09:25:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/03/21 09:25:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bdd1d508-c6c1-4dfb-aa68-d45415499c93\n",
      "22/03/21 09:25:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "22/03/21 09:25:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/03/21 09:25:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/03/21 09:25:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-12-99.eu-central-1.compute.internal:4040\n",
      "22/03/21 09:25:21 INFO SparkContext: Added JAR file:/home/ubuntu/spark-3.2.1-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.2.1.jar at spark://ip-172-31-12-99.eu-central-1.compute.internal:39479/jars/spark-examples_2.12-3.2.1.jar with timestamp 1647854720806\n",
      "22/03/21 09:25:22 INFO Executor: Starting executor ID driver on host ip-172-31-12-99.eu-central-1.compute.internal\n",
      "22/03/21 09:25:22 INFO Executor: Fetching spark://ip-172-31-12-99.eu-central-1.compute.internal:39479/jars/spark-examples_2.12-3.2.1.jar with timestamp 1647854720806\n",
      "22/03/21 09:25:22 INFO TransportClientFactory: Successfully created connection to ip-172-31-12-99.eu-central-1.compute.internal/172.31.12.99:39479 after 31 ms (0 ms spent in bootstraps)\n",
      "22/03/21 09:25:22 INFO Utils: Fetching spark://ip-172-31-12-99.eu-central-1.compute.internal:39479/jars/spark-examples_2.12-3.2.1.jar to /tmp/spark-9d6b0721-b432-4bc7-989f-0f2fa2cd019e/userFiles-40881b47-574f-49ea-9900-89d7b80ae7ef/fetchFileTemp1768667875666947942.tmp\n",
      "22/03/21 09:25:22 INFO Executor: Adding file:/tmp/spark-9d6b0721-b432-4bc7-989f-0f2fa2cd019e/userFiles-40881b47-574f-49ea-9900-89d7b80ae7ef/spark-examples_2.12-3.2.1.jar to class loader\n",
      "22/03/21 09:25:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46031.\n",
      "22/03/21 09:25:22 INFO NettyBlockTransferService: Server created on ip-172-31-12-99.eu-central-1.compute.internal:46031\n",
      "22/03/21 09:25:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/03/21 09:25:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46031, None)\n",
      "22/03/21 09:25:22 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-12-99.eu-central-1.compute.internal:46031 with 366.3 MiB RAM, BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46031, None)\n",
      "22/03/21 09:25:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46031, None)\n",
      "22/03/21 09:25:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-12-99.eu-central-1.compute.internal, 46031, None)\n",
      "22/03/21 09:25:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/03/21 09:25:22 INFO SharedState: Warehouse path is 'file:/home/ubuntu/sample-code-benchmark/sample-code/word-count/spark-warehouse'.\n",
      "22/03/21 09:25:23 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.\n",
      "22/03/21 09:25:26 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/03/21 09:25:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/03/21 09:25:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/03/21 09:25:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 338.4 KiB, free 366.0 MiB)\n",
      "22/03/21 09:25:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)\n",
      "22/03/21 09:25:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46031 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "22/03/21 09:25:26 INFO SparkContext: Created broadcast 0 from javaRDD at JavaWordCount.java:45\n",
      "22/03/21 09:25:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4199225 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/03/21 09:25:26 INFO SparkContext: Starting job: collect at JavaWordCount.java:53\n",
      "22/03/21 09:25:26 INFO DAGScheduler: Registering RDD 6 (mapToPair at JavaWordCount.java:49) as input to shuffle 0\n",
      "22/03/21 09:25:26 INFO DAGScheduler: Got job 0 (collect at JavaWordCount.java:53) with 1 output partitions\n",
      "22/03/21 09:25:26 INFO DAGScheduler: Final stage: ResultStage 1 (collect at JavaWordCount.java:53)\n",
      "22/03/21 09:25:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "22/03/21 09:25:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "22/03/21 09:25:26 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at mapToPair at JavaWordCount.java:49), which has no missing parents\n",
      "22/03/21 09:25:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.3 KiB, free 365.9 MiB)\n",
      "22/03/21 09:25:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 365.9 MiB)\n",
      "22/03/21 09:25:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46031 (size: 7.8 KiB, free: 366.3 MiB)\n",
      "22/03/21 09:25:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "22/03/21 09:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at mapToPair at JavaWordCount.java:49) (first 15 tasks are for partitions Vector(0))\n",
      "22/03/21 09:25:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/03/21 09:25:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ip-172-31-12-99.eu-central-1.compute.internal, executor driver, partition 0, PROCESS_LOCAL, 4897 bytes) taskResourceAssignments Map()\n",
      "22/03/21 09:25:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/03/21 09:25:28 INFO CodeGenerator: Code generated in 274.832647 ms\n",
      "22/03/21 09:25:28 INFO FileScanRDD: Reading File path: file:///home/ubuntu/sample-code-benchmark/sample-code/word-count/data/wordcount.txt, range: 0-4921, partition values: [empty row]\n",
      "22/03/21 09:25:28 INFO CodeGenerator: Code generated in 20.491219 ms\n",
      "22/03/21 09:25:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1826 bytes result sent to driver\n",
      "22/03/21 09:25:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1161 ms on ip-172-31-12-99.eu-central-1.compute.internal (executor driver) (1/1)\n",
      "22/03/21 09:25:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/03/21 09:25:28 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at JavaWordCount.java:49) finished in 1.408 s\n",
      "22/03/21 09:25:28 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/03/21 09:25:28 INFO DAGScheduler: running: Set()\n",
      "22/03/21 09:25:28 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "22/03/21 09:25:28 INFO DAGScheduler: failed: Set()\n",
      "22/03/21 09:25:28 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[7] at reduceByKey at JavaWordCount.java:51), which has no missing parents\n",
      "22/03/21 09:25:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.2 KiB, free 365.9 MiB)\n",
      "22/03/21 09:25:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.9 KiB, free 365.9 MiB)\n",
      "22/03/21 09:25:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-172-31-12-99.eu-central-1.compute.internal:46031 (size: 2.9 KiB, free: 366.3 MiB)\n",
      "22/03/21 09:25:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
      "22/03/21 09:25:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[7] at reduceByKey at JavaWordCount.java:51) (first 15 tasks are for partitions Vector(0))\n",
      "22/03/21 09:25:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/03/21 09:25:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ip-172-31-12-99.eu-central-1.compute.internal, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "22/03/21 09:25:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/03/21 09:25:28 INFO ShuffleBlockFetcherIterator: Getting 1 (3.9 KiB) non-empty blocks including 1 (3.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/03/21 09:25:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "22/03/21 09:25:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 9501 bytes result sent to driver\n",
      "22/03/21 09:25:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 170 ms on ip-172-31-12-99.eu-central-1.compute.internal (executor driver) (1/1)\n",
      "22/03/21 09:25:28 INFO DAGScheduler: ResultStage 1 (collect at JavaWordCount.java:53) finished in 0.191 s\n",
      "22/03/21 09:25:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/03/21 09:25:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/03/21 09:25:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "22/03/21 09:25:28 INFO DAGScheduler: Job 0 finished: collect at JavaWordCount.java:53, took 1.711352 s\n",
      "captions: 1\n",
      "Software: 2\n",
      "it: 2\n",
      "others: 1\n",
      "todays: 1\n",
      "outside: 1\n",
      "improve: 1\n",
      "counted: 2\n",
      "include: 1\n",
      "definition: 3\n",
      "hidden: 2\n",
      "could: 1\n",
      "result: 1\n",
      "barring: 1\n",
      "exact: 1\n",
      "applications: 2\n",
      "over: 2\n",
      "used: 4\n",
      "To: 1\n",
      "any: 2\n",
      "requirement: 1\n",
      "application: 1\n",
      "select: 1\n",
      "wc: 1\n",
      "these: 2\n",
      "determined: 1\n",
      "compounds: 1\n",
      "books: 1\n",
      "are: 4\n",
      "different: 1\n",
      "em: 1\n",
      "determine: 1\n",
      "most: 3\n",
      "dissertation: 1\n",
      "be: 8\n",
      "Novelette: 1\n",
      "minute: 1\n",
      "price: 1\n",
      "including: 1\n",
      "segmentation: 4\n",
      "story: 1\n",
      "toward: 2\n",
      "generally: 2\n",
      "divider: 1\n",
      "thus: 1\n",
      "arbitrary: 1\n",
      "but: 2\n",
      "firm: 1\n",
      "slash: 1\n",
      "conjunctions: 1\n",
      "namely: 2\n",
      "Variations: 1\n",
      "per: 3\n",
      "during: 1\n",
      "Sources: 1\n",
      "prepositions: 1\n",
      "wayside: 1\n",
      "long: 1\n",
      "But: 2\n",
      "found: 1\n",
      "citations: 1\n",
      "person: 1\n",
      "sometimes: 1\n",
      "Unsourced: 1\n",
      "while: 3\n",
      "certain: 2\n",
      "extent: 1\n",
      "at: 3\n",
      "broadly: 1\n",
      "sources: 2\n",
      "total: 2\n",
      "speeds: 1\n",
      "hyphen: 1\n",
      "adding: 1\n",
      "children: 1\n",
      "section: 2\n",
      "Most: 2\n",
      "bottom: 1\n",
      "now: 1\n",
      "bibliographies: 1\n",
      "give: 2\n",
      "10: 1\n",
      "may: 8\n",
      "typically: 1\n",
      "don't: 2\n",
      "adjective: 1\n",
      "via: 2\n",
      "sections: 1\n",
      "assignments: 1\n",
      "effort: 1\n",
      "main: 1\n",
      "drilled: 1\n",
      "or: 11\n",
      "As: 2\n",
      "strict: 1\n",
      "requires: 1\n",
      "behavior: 1\n",
      "academia: 1\n",
      "six: 1\n",
      "of: 25\n",
      "does: 1\n",
      "hosted: 1\n",
      "consensus: 3\n",
      "often: 3\n",
      "reference: 1\n",
      "importantly: 1\n",
      "varies: 1\n",
      "broad: 1\n",
      "variation: 1\n",
      "this: 2\n",
      "Different: 1\n",
      "results: 2\n",
      "noun: 1\n",
      "earlier: 2\n",
      "Contents: 1\n",
      "each: 1\n",
      "some: 1\n",
      "tremendously: 1\n",
      "Writers: 1\n",
      "first: 1\n",
      "range: 1\n",
      "suggests: 1\n",
      "definitions: 3\n",
      "See: 1\n",
      "exceeding: 1\n",
      "needed: 1\n",
      "for: 10\n",
      "already: 1\n",
      "rules: 5\n",
      "time: 1\n",
      "Smiley: 2\n",
      "too: 1\n",
      "encyclopedia: 1\n",
      "example: 2\n",
      "calculate: 1\n",
      "measure: 2\n",
      "fiction: 4\n",
      "was: 1\n",
      "processors: 1\n",
      "how: 3\n",
      "no: 1\n",
      "When: 1\n",
      "whitespace: 1\n",
      "there: 1\n",
      "Hyphenated: 1\n",
      "references: 1\n",
      "thriller: 1\n",
      "count: 11\n",
      "abstracts: 1\n",
      "words: 21\n",
      "accept: 1\n",
      "quality: 1\n",
      "elements: 1\n",
      "greatly: 1\n",
      "start: 1\n",
      "number: 3\n",
      "links: 1\n",
      "simply: 1\n",
      "teacher: 1\n",
      "one: 1\n",
      "help: 1\n",
      "might: 1\n",
      "integer: 1\n",
      "novella: 1\n",
      "in: 11\n",
      "word: 24\n",
      "update: 1\n",
      "feature: 1\n",
      "define: 1\n",
      "from: 2\n",
      "dependent: 1\n",
      "Numerous: 1\n",
      "specifically: 1\n",
      "passage: 1\n",
      "counts: 3\n",
      "JavaScript: 1\n",
      "that: 5\n",
      "consistent: 1\n",
      "many: 2\n",
      "between: 2\n",
      "follow: 1\n",
      "National: 1\n",
      "widespread: 1\n",
      "translation: 1\n",
      "choice: 1\n",
      "students: 2\n",
      "advertising: 1\n",
      "especially: 1\n",
      "follows: 1\n",
      "users: 1\n",
      "Unix: 1\n",
      "case: 1\n",
      "characters: 2\n",
      "universities: 1\n",
      "commonly: 1\n",
      "predominantly: 1\n",
      "included: 1\n",
      "specifies: 1\n",
      "under: 1\n",
      "Fantasy: 1\n",
      "Fiction: 1\n",
      "The: 5\n",
      "Short: 1\n",
      "translators: 1\n",
      "its: 3\n",
      "legal: 1\n",
      "Novelist: 1\n",
      "reading: 1\n",
      "acceptable: 2\n",
      "literary: 1\n",
      "have: 1\n",
      "browsers: 1\n",
      "line: 3\n",
      "such: 7\n",
      "reliable: 1\n",
      "differed: 1\n",
      "dividers: 1\n",
      "This: 2\n",
      "workers: 1\n",
      "extensions: 1\n",
      "automatically: 1\n",
      "software: 2\n",
      "largely: 1\n",
      "articles: 1\n",
      "varying: 2\n",
      "required: 1\n",
      "text): 1\n",
      "the: 38\n",
      "free: 1\n",
      "Details: 2\n",
      "not: 2\n",
      "Ph.D.: 1\n",
      "typewriters: 1\n",
      "when: 2\n",
      "chapter: 1\n",
      "accordingly: 1\n",
      "fallen: 1\n",
      "as: 9\n",
      "average: 1\n",
      "writer: 1\n",
      "usually: 3\n",
      "program: 1\n",
      "Modern: 1\n",
      "space: 3\n",
      "lists: 2\n",
      "Please: 1\n",
      "boundaries: 1\n",
      "is: 19\n",
      "about: 1\n",
      "charge: 1\n",
      "least: 1\n",
      "text: 8\n",
      "permission: 1\n",
      "on: 7\n",
      "exclude: 1\n",
      "across: 1\n",
      "Word: 3\n",
      "variations: 2\n",
      "trait: 1\n",
      "stay: 1\n",
      "effortless: 1\n",
      "handwriting: 1\n",
      "footnotes: 2\n",
      "script: 1\n",
      "similar: 1\n",
      "special: 1\n",
      "subject: 1\n",
      "details: 2\n",
      "Month: 1\n",
      "academic: 1\n",
      "Novel: 2\n",
      "operational: 2\n",
      "boundary: 1\n",
      "processing: 4\n",
      "Nebula: 1\n",
      "Usually: 1\n",
      "thumb: 1\n",
      "five: 1\n",
      "documents: 1\n",
      "counting: 6\n",
      "occur: 1\n",
      "These: 1\n",
      "Novella: 1\n",
      "job: 1\n",
      "term: 1\n",
      "There: 1\n",
      "dissertations: 1\n",
      "which: 4\n",
      "standard: 1\n",
      "also: 5\n",
      "whether: 1\n",
      "character: 2\n",
      "what: 1\n",
      "particularly: 1\n",
      "difficult: 1\n",
      "America: 1\n",
      "vary: 2\n",
      "converting: 1\n",
      "rule: 4\n",
      "Classification\tWord: 1\n",
      "up: 2\n",
      "typing: 1\n",
      "bookmarklet: 1\n",
      "typical: 1\n",
      "can: 5\n",
      "limit: 2\n",
      "proceedings: 1\n",
      "challenged: 1\n",
      "non: 2\n",
      "categorise: 1\n",
      "regular: 1\n",
      "client: 1\n",
      "done: 1\n",
      "material: 1\n",
      "since: 1\n",
      "tab: 1\n",
      "journalism: 1\n",
      "figure: 1\n",
      "by: 5\n",
      "like: 1\n",
      "defined: 1\n",
      "readability: 1\n",
      "categories: 1\n",
      "cite: 1\n",
      "document: 2\n",
      "However: 3\n",
      "an: 4\n",
      "References: 1\n",
      "endnotes: 2\n",
      "systems: 1\n",
      "arbiter: 1\n",
      "save: 1\n",
      "major: 1\n",
      "website: 1\n",
      "depends: 1\n",
      "American: 1\n",
      "mentioned: 1\n",
      "award: 1\n",
      "with: 2\n",
      "category: 1\n",
      "novels: 3\n",
      "In: 4\n",
      "tables: 1\n",
      "explained: 1\n",
      "costless: 1\n",
      "support: 1\n",
      "numbers: 1\n",
      "depending: 2\n",
      "permanent: 1\n",
      "hence: 1\n",
      "Jane: 1\n",
      "within: 1\n",
      "removed: 1\n",
      "advent: 1\n",
      "Science: 1\n",
      "mystery: 1\n",
      "because: 2\n",
      "various: 1\n",
      "a: 28\n",
      "fast: 1\n",
      "their: 1\n",
      "work: 1\n",
      "large: 1\n",
      "novel: 4\n",
      "to: 18\n",
      "era: 2\n",
      "External: 1\n",
      "school: 1\n",
      "Writing: 1\n",
      "web: 1\n",
      "important: 1\n",
      "lengths: 2\n",
      "programs: 2\n",
      "watch: 1\n",
      "Wikipedia: 1\n",
      "were: 3\n",
      "length: 5\n",
      "and: 23\n",
      "subgenre: 1\n",
      "being: 1\n",
      "measures: 1\n",
      "22/03/21 09:25:28 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-12-99.eu-central-1.compute.internal:4040\n",
      "22/03/21 09:25:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/03/21 09:25:28 INFO MemoryStore: MemoryStore cleared\n",
      "22/03/21 09:25:28 INFO BlockManager: BlockManager stopped\n",
      "22/03/21 09:25:28 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/03/21 09:25:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/03/21 09:25:28 INFO SparkContext: Successfully stopped SparkContext\n",
      "22/03/21 09:25:28 INFO ShutdownHookManager: Shutdown hook called\n",
      "22/03/21 09:25:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0145918-dc4d-4bfc-a2e6-4f297a1ef1b8\n",
      "22/03/21 09:25:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d6b0721-b432-4bc7-989f-0f2fa2cd019e\n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/bin/spark-submit \\\n",
    "    --master local \\\n",
    "    --class org.apache.spark.examples.JavaWordCount $SPARK_HOME/examples/jars/spark-examples_2.12-3.2.1.jar \\\n",
    "    ./data/wordcount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f08440-bcd6-4e68-a2fa-f94c9a855baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2df3d0-674f-48d1-9ab5-52553db84891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22866a-36b2-43fa-92ad-614d68f019d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
